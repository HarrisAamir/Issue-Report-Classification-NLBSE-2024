{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Install Related Libraries**"
      ],
      "metadata": {
        "id": "mpCtskUWh8vd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SUHrF5GJij4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.17"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F6_CncX4OuIg"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "import torch\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import ast\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, precision_recall_fscore_support,accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Datasets**"
      ],
      "metadata": {
        "id": "nQHnS5iOimLW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RBORJMLkKESc"
      },
      "outputs": [],
      "source": [
        "trainingData=pd.read_csv(\"issues_train.csv\")\n",
        "testingData=pd.read_csv(\"issues_test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "8LsPdCJ5i0qA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LbjQcw2IVGC6"
      },
      "outputs": [],
      "source": [
        "trainingData[\"summary\"]=trainingData[\"title\"]+\" \"+trainingData['body']\n",
        "testingData[\"summary\"]=testingData[\"title\"]+\" \"+trainingData['body']\n",
        "trainingData[\"summary\"]=trainingData[\"summary\"].apply(lambda x: x[:256] if len(str(x))>256 else x)\n",
        "testingData[\"summary\"]=testingData[\"summary\"].apply(lambda x: x[:256] if len(str(x))>256 else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6t7O27FqN1jw"
      },
      "outputs": [],
      "source": [
        "def checkLink(text):\n",
        "    link_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    return re.sub(link_pattern, \"LINK\" ,str(text))\n",
        "\n",
        "trainingData['summary'] = trainingData['summary'].apply(checkLink)\n",
        "testingData['summary'] = testingData['summary'].apply(checkLink)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tmTT_72ZOHWt"
      },
      "outputs": [],
      "source": [
        "trainingData = trainingData.applymap(lambda x: x.lower() if isinstance(x, str) else str(x))\n",
        "testingData = testingData.applymap(lambda x: x.lower() if isinstance(x, str) else str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dSwhaZ06OLAJ"
      },
      "outputs": [],
      "source": [
        "def removeSpecialCharacters(text):\n",
        "    pattern = r'[^a-zA-Z]'\n",
        "    cleanedText = re.sub(pattern, ' ', text)\n",
        "    cleanedText = ' '.join(cleanedText.split())\n",
        "    return cleanedText\n",
        "\n",
        "trainingData['summary'] = trainingData['summary'].apply(lambda x :removeSpecialCharacters(str(x)))\n",
        "testingData['summary'] = testingData['summary'].apply(lambda x :removeSpecialCharacters(str(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PooBekvOV4y"
      },
      "outputs": [],
      "source": [
        "def dropNan(x):\n",
        "  if len(str(x))<5:\n",
        "    return False\n",
        "  else: return True\n",
        "\n",
        "mask=trainingData[\"summary\"].apply(dropNan)\n",
        "trainingData=trainingData[mask]\n",
        "mask=testingData[\"summary\"].apply(dropNan)\n",
        "testingData=testingData[mask]\n",
        "trainingData = trainingData.drop_duplicates(subset='summary')\n",
        "testingData = testingData.drop_duplicates(subset='summary')\n",
        "print(trainingData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LH2Z70WiOh3s"
      },
      "outputs": [],
      "source": [
        "trainingData=trainingData[['repo','summary','label']]\n",
        "testingData=testingData[['repo','summary','label']]\n",
        "trainingData.to_csv(\"training_Dataset.csv\", index=False)\n",
        "testingData.to_csv(\"testing_Dataset.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset Preparation**"
      ],
      "metadata": {
        "id": "LEogPf6ojgsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lquI2HOHPB5A"
      },
      "outputs": [],
      "source": [
        "#load and view train data\n",
        "data = pd.read_csv(\"training_Dataset.csv\")\n",
        "\n",
        "label=[]\n",
        "for index, row in data.iterrows():\n",
        "  if row['label']==\"bug\":\n",
        "     label.append(0)\n",
        "  elif row['label']==\"feature\":\n",
        "      label.append(1)\n",
        "  else: label.append(2)\n",
        "data['label']=label\n",
        "\n",
        "X = list(data[\"summary\"])\n",
        "y = list(data[\"label\"])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)\n",
        "df_train = pd.DataFrame({\"summary\":X_train,\"label\":y_train})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X0i-w--POrT"
      },
      "outputs": [],
      "source": [
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-large',num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jI9s4OCiPV-G"
      },
      "outputs": [],
      "source": [
        "df_valid = pd.DataFrame({\"summary\":X_val,\"label\":y_val})\n",
        "train_text = df_train.summary.values\n",
        "train_label = df_train.label.values\n",
        "val_text = df_valid.summary.values\n",
        "val_label = df_valid.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2RF36GPvPhr3"
      },
      "outputs": [],
      "source": [
        "# define custom dataset\n",
        "class TextClassificationDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = self.texts[index]\n",
        "        label = self.labels[index]\n",
        "\n",
        "        encoded_text = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=256,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=False,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encoded_text['input_ids'].squeeze()\n",
        "        attention_mask = encoded_text['attention_mask'].squeeze()\n",
        "        label = torch.tensor(label)\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.cpu(),\n",
        "            'attention_mask': attention_mask.cpu(),\n",
        "            'labels': label.cpu()\n",
        "        }\n",
        "\n",
        "# create datasets\n",
        "train_dataset = TextClassificationDataset(train_text, train_label, tokenizer)\n",
        "eval_dataset = TextClassificationDataset(val_text, val_label, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XF0nYvotPjrN"
      },
      "outputs": [],
      "source": [
        "#define custom metrics for validation to avoid error\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Hyperparameters**"
      ],
      "metadata": {
        "id": "XJjoeKlDkBaE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ontXnKy9PmQg"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=32,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=5,\n",
        "    weight_decay=32,\n",
        "    fp16=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=7e-6,\n",
        "    greater_is_better=True,\n",
        "    gradient_accumulation_steps=2,\n",
        "    eval_steps=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEseu8l8Pq4s"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Training**"
      ],
      "metadata": {
        "id": "mlsYTl1qkUSZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ4LD-OwPtzs"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "keXYVOamkzJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH6V7z3DPwI0"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Testing**"
      ],
      "metadata": {
        "id": "189jrhH8k6uU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMh5b44VPyde"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv(\"testing_Dataset.csv\")\n",
        "test_data.dropna(inplace=True)\n",
        "X_test = list(test_data[\"summary\"])\n",
        "label=[]\n",
        "for index, row in test_data.iterrows():\n",
        "  if row['label']==\"bug\":\n",
        "     label.append(0)\n",
        "  elif row['label']==\"feature\":\n",
        "      label.append(1)\n",
        "  else: label.append(2)\n",
        "test_data['label']=label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ylv-LzyQQAT3"
      },
      "outputs": [],
      "source": [
        "test_dataset = TextClassificationDataset(test_data['summary'],test_data['label'],tokenizer)\n",
        "predictions=trainer.predict(test_dataset=test_dataset).predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWVsSEHIQT7u"
      },
      "outputs": [],
      "source": [
        "labels=[\"bug\",\"feature\",\"question\"]\n",
        "predicted_labels=[]\n",
        "trueFalse=[]\n",
        "i=0\n",
        "for prediction in predictions:\n",
        "  index_of_max = np.argmax(prediction)\n",
        "  predicted_labels.append(index_of_max)\n",
        "\n",
        "test_data[\"predicted_label\"]=predicted_labels\n",
        "\n",
        "for index, row in test_data.iterrows():\n",
        "  if row['label']==row['predicted_label']:\n",
        "     trueFalse.append(\"True\")\n",
        "  else: trueFalse.append(\"False\")\n",
        "\n",
        "test_data[\"True/False\"]=trueFalse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRGgjCbjQXHm"
      },
      "outputs": [],
      "source": [
        "actual = test_data['label']\n",
        "predicted = test_data['predicted_label']\n",
        "accuracy = accuracy_score(actual, predicted)\n",
        "precision = precision_score(actual, predicted, average='weighted')\n",
        "recall = recall_score(actual, predicted ,average='weighted')\n",
        "f1 = f1_score(actual, predicted, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Results**"
      ],
      "metadata": {
        "id": "QIsBOJw3lIAM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-grsP0wQi6l"
      },
      "outputs": [],
      "source": [
        "repos = list(set(test_data[\"repo\"].unique()))\n",
        "grouped = test_data.groupby(\"repo\")\n",
        "smaller_dataframes = {}\n",
        "for group_name, group_data in grouped:\n",
        "    smaller_dataframes[group_name] = group_data.copy()\n",
        "\n",
        "for repo in repos:\n",
        "  precision, recall, f1, support = precision_recall_fscore_support(smaller_dataframes[repo]['label'],smaller_dataframes[repo][\"predicted_label\"])\n",
        "  print(\"---------\",repo,\"------------\")\n",
        "  for label in range(len(precision)):\n",
        "    print(f\"Label {labels[label]}:\")\n",
        "    print(f\"Precision: {precision[label]}\")\n",
        "    print(f\"Recall: {recall[label]}\")\n",
        "    print(f\"F1 Score: {f1[label]}\")\n",
        "    print(f\"support: {support[label]}\")\n",
        "  print(f\"Average Precision: {np.average(precision)}\")\n",
        "  print(f\"Average Recall: {np.average(recall)}\")\n",
        "  print(f\"Average F1 Score: {np.average(f1)}\")\n",
        "  print(f\"Average Support: {np.average(support)}\")\n",
        "  print(\"---------------------\")\n",
        "  print(\"_______________________________________\")\n",
        "  i=i+1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}